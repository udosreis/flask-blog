Is Technical Interview Performance is Arbitrary?
Ugo Dos Reis
2020-11-15
<p>In the article "Technical interview performance is kind of arbitrary. Here's the data," the author discusses why technical interviews are not a proper way to choose a candidate for a job. Although the graphics in the article are good, the paper is plagued by unprofessional writing, while the study ignores crucial variables when making conclusions. 

<p>In the article, the author explains their data is from the "interviewing.io" platform, which is a website where people can practice technical interviews. With this data, the author presents multiple graphs showing the interviewees' average interview score out of 4. The author then presents other charts that display the probability of failing an interview based on one's average score. Finally, the author concludes that the volatility of the scores implies that technical interviews are meaningless, as a bad interviewee can do well out of the blue while a good interviewee can perform well below their average. 

<p>Despite having intriguing results, the paper's credibility is shattered because of how it is written. The article uses the word "meager" to describe the data collected but also uses that same data to come to conclusions. If the author believes their data is lacking in quality/quantity, why do they draw conclusions from it before gathering more data? As a reader, having the writer make conclusions from data they consider "meager" damages how much I believe in the results. Other than that, the article uses the swear "fuck" a couple of times, taking the reader by complete surprise. Overall, the overly casual wording of the article, and the way it addresses the results makes the paper very questionable.

<p>Apart from the wording needing revision, the article is also brought down because of the study. The study looked at 299 interviews, but only 67 interviewees. While this might seem like a decent ratio (4.5 interviews per person), the data includes multiple people who only took two interviews, meaning the data may be skewed. It is possible that an interviewee took their two interviews in a very spread out manner and got better in between, or they could have had no interview experience in their first interview and may have prepared better for their second. With this in mind, the study fails to show the growth of candidates. There are a couple of candidates who took many interviews with different scores, but we have no information as to whether they scored a bunch of 2s, then a bunch of 3s, and then 4s, or whether the scores were spread out. It would make sense that as an interviewee practices more and more, they get used to the type of questions and format, leading to them doing better. Alas, the article uses the data without giving timeframes meaning that readers can't know whether the conclusions make sense. 

<p>All in all, the article's methodology and wording make it hard to believe the results. The article describes their data as lacking and uses swears throughout, making it hard to take it seriously. The study also ignores important variables like timeframes when considering the data, rendering the results meaningless. Overall, for increased credibility, the study needs to become more specific, and the article needs to be completely reworded.